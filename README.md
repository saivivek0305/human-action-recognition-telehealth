# Human Action Recognition in Telehealth

A deep learningâ€“based **Human Action Recognition (HAR)** system designed for telehealth environments.  
This project detects simple human movements and actions using a **CNN + LSTM architecture**, supported by a custom synthetic dataset of 15 videos.

It demonstrates how AI can be used in remote healthcare to monitor patient activity, identify movements, and support automated alerts without continuous human supervision.

---

## ðŸ§  Project Overview

Traditional telehealth systems lack real-time action monitoring.  
This project addresses that by providing:

- A lightweight action-recognition model  
- Clean preprocessing pipeline  
- A synthetic dataset for training/testing  
- Training, inference, and evaluation scripts  
- Modular, production-ready folder structure  

Built for students, researchers, and ML beginners looking to understand HAR.

---

## ðŸš€ Features

- Custom dataset of **15 synthetic action videos**:
  - Stick-figure walking  
  - Silhouette exercise actions  
  - Synthetic moving shapes  

- Automated **frame extraction**  
- CNN backbone + LSTM temporal modeling  
- Inference script to test any video  
- Annotation CSV for dataset labeling  
- Modular code structure for easy extension  

---

## ðŸ“‚ Project Structure
human-action-recognition-telehealth/
â”‚
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ preprocess.py # Extract frames from videos
â”‚ â”œâ”€â”€ train.py # Train CNN-LSTM model
â”‚ â””â”€â”€ infer.py # Run inference on new videos
â”‚
â”œâ”€â”€ models/
â”‚ â””â”€â”€ simple_cnn_lstm.py # CNN + LSTM architecture
â”‚
â”œâ”€â”€ examples/
â”‚ â”œâ”€â”€ sample_video.mp4
â”‚ â””â”€â”€ videos/
â”‚ â”œâ”€â”€ synthetic/
â”‚ â”œâ”€â”€ stick_walk/
â”‚ â””â”€â”€ silhouette_exercise/
â”‚
â”œâ”€â”€ annotations.csv # Video â†’ label mapping
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ LICENSE
â””â”€â”€ README.md

